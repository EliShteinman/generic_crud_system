# ××“×¨×™×š ××¢×•×“×›×Ÿ: PyMongo ××¡×™× ×›×¨×•× ×™ + Pandas + MongoDB Atlas

## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×
1. [×—×™×‘×•×¨ ×œ-MongoDB Atlas ×¢× PyMongo ××¡×™× ×›×¨×•× ×™](#×—×™×‘×•×¨-×œ-mongodb-atlas)
2. [DAL ××¢×©×™ ×œ××˜×œ×¡](#dal-××¢×©×™-×œ××˜×œ×¡)
3. [×§×¨×™××ª × ×ª×•× ×™× - ×¡×™× ×›×¨×•× ×™ vs ××¡×™× ×›×¨×•× ×™](#×§×¨×™××ª-× ×ª×•× ×™×)
4. [×”××¨×” ×œ-Pandas](#×”××¨×”-×œ-pandas)
5. [×“×™×¤×“×•×£ (Pagination) ×‘××¡××›×™×](#×“×™×¤×“×•×£-×‘××¡××›×™×)
6. [××“×¨×™×š Pandas ×œ××‘×—×Ÿ](#××“×¨×™×š-pandas-×œ××‘×—×Ÿ)

---

## ×—×™×‘×•×¨ ×œ-MongoDB Atlas

### PyMongo ××•×“×¨× ×™ (4.0+) - ×¡×™× ×›×¨×•× ×™ ×•××¡×™× ×›×¨×•× ×™

```python
# ×”×ª×§× ×”
pip install pymongo[srv] pandas

# ×™×™×‘×•×
from pymongo import MongoClient
from pymongo.server_api import ServerApi
import pandas as pd
import asyncio
from typing import List, Dict, Any, Optional
```

### ×—×™×‘×•×¨ ×œ-Atlas (×¡×™× ×›×¨×•× ×™)
```python
class MongoAtlasConnection:
    def __init__(self, connection_string: str, database_name: str):
        """
        ×—×™×‘×•×¨ ×œ-MongoDB Atlas
        connection_string: mongodb+srv://username:password@cluster.mongodb.net/?retryWrites=true&w=majority
        """
        self.connection_string = connection_string
        self.database_name = database_name
        self.client = None
        self.db = None
    
    def connect(self):
        """×™×¦×™×¨×ª ×—×™×‘×•×¨ ×œ-Atlas"""
        try:
            self.client = MongoClient(
                self.connection_string,
                server_api=ServerApi('1'),  # ×’×¨×¡×” ×™×¦×™×‘×”
                serverSelectionTimeoutMS=5000,
                connectTimeoutMS=5000,
                maxPoolSize=50,
                retryWrites=True
            )
            
            # ×‘×“×™×§×ª ×—×™×‘×•×¨
            self.client.admin.command('ping')
            print("âœ… ×—×™×‘×•×¨ ×œ-MongoDB Atlas ×”×¦×œ×™×—!")
            
            self.db = self.client[self.database_name]
            return True
            
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×—×™×‘×•×¨ ×œ-Atlas: {e}")
            return False
    
    def get_collection(self, collection_name: str):
        """×§×‘×œ×ª ×§×•×œ×§×©×Ÿ"""
        if self.db is None:
            raise RuntimeError("×œ× ××—×•×‘×¨ ×œ××¡×“ × ×ª×•× ×™×")
        return self.db[collection_name]
    
    def disconnect(self):
        """× ×™×ª×•×§"""
        if self.client:
            self.client.close()
            print("ğŸ”Œ ×”×ª× ×ª×§×•×ª ×-Atlas ×”×•×©×œ××”")

# ×©×™××•×©:
atlas = MongoAtlasConnection(
    "mongodb+srv://username:password@cluster.mongodb.net/?retryWrites=true&w=majority",
    "school_db"
)

if atlas.connect():
    students_collection = atlas.get_collection("students")
```

### ×—×™×‘×•×¨ ××¡×™× ×›×¨×•× ×™ (×¢× AsyncIOMotorClient)
```python
from motor.motor_asyncio import AsyncIOMotorClient

class AsyncMongoAtlas:
    def __init__(self, connection_string: str, database_name: str):
        self.connection_string = connection_string
        self.database_name = database_name
        self.client = None
        self.db = None
    
    async def connect(self):
        """×—×™×‘×•×¨ ××¡×™× ×›×¨×•× ×™"""
        try:
            self.client = AsyncIOMotorClient(
                self.connection_string,
                serverSelectionTimeoutMS=5000,
                maxPoolSize=50
            )
            
            # ×‘×“×™×§×ª ×—×™×‘×•×¨
            await self.client.admin.command('ping')
            print("âœ… ×—×™×‘×•×¨ ××¡×™× ×›×¨×•× ×™ ×œ-Atlas ×”×¦×œ×™×—!")
            
            self.db = self.client[self.database_name]
            return True
            
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×—×™×‘×•×¨ ××¡×™× ×›×¨×•× ×™: {e}")
            return False
    
    def get_collection(self, collection_name: str):
        if self.db is None:
            raise RuntimeError("×œ× ××—×•×‘×¨ ×œ××¡×“ × ×ª×•× ×™×")
        return self.db[collection_name]
    
    async def disconnect(self):
        if self.client:
            self.client.close()

# ×©×™××•×©:
async def main():
    atlas = AsyncMongoAtlas(
        "mongodb+srv://username:password@cluster.mongodb.net/",
        "school_db"
    )
    
    if await atlas.connect():
        collection = atlas.get_collection("students")
        # ×¢×‘×•×“×” ×¢× ×”×§×•×œ×§×©×Ÿ...
        await atlas.disconnect()

# ×”×¨×¦×”
asyncio.run(main())
```

---

## DAL ××¢×©×™ ×œ××˜×œ×¡

```python
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime

class AtlasDAL:
    """×©×›×‘×ª ×’×™×©×” ×œ× ×ª×•× ×™× ×œ-MongoDB Atlas"""
    
    def __init__(self, connection_string: str, database_name: str):
        self.atlas = MongoAtlasConnection(connection_string, database_name)
        self.connected = False
    
    def connect(self) -> bool:
        """×”×ª×—×‘×¨×•×ª ×œ××¡×“"""
        self.connected = self.atlas.connect()
        return self.connected
    
    def _ensure_connected(self):
        """×‘×“×™×§×” ×©×™×© ×—×™×‘×•×¨"""
        if not self.connected:
            raise RuntimeError("DAL ×œ× ××—×•×‘×¨ ×œ××¡×“ × ×ª×•× ×™×")
    
    def find_documents(self, collection_name: str, 
                      query: Dict[str, Any] = None,
                      projection: Dict[str, int] = None,
                      sort: List[tuple] = None,
                      limit: int = None,
                      skip: int = 0) -> List[Dict[str, Any]]:
        """××¦×™××ª ××¡××›×™× ×¢× ××¤×©×¨×•×™×•×ª ××œ××•×ª"""
        
        self._ensure_connected()
        collection = self.atlas.get_collection(collection_name)
        
        query = query or {}
        cursor = collection.find(query, projection)
        
        if sort:
            cursor = cursor.sort(sort)
        if skip > 0:
            cursor = cursor.skip(skip)
        if limit:
            cursor = cursor.limit(limit)
        
        # ×”××¨×” ×œ×¨×©×™××” + × ×™×§×•×™ ObjectId
        documents = []
        for doc in cursor:
            if '_id' in doc:
                doc['_id'] = str(doc['_id'])
            documents.append(doc)
        
        return documents
    
    def find_one_document(self, collection_name: str,
                         query: Dict[str, Any],
                         projection: Dict[str, int] = None) -> Optional[Dict[str, Any]]:
        """××¦×™××ª ××¡××š ××—×“"""
        
        self._ensure_connected()
        collection = self.atlas.get_collection(collection_name)
        
        doc = collection.find_one(query, projection)
        if doc and '_id' in doc:
            doc['_id'] = str(doc['_id'])
        
        return doc
    
    def count_documents(self, collection_name: str,
                       query: Dict[str, Any] = None) -> int:
        """×¡×¤×™×¨×ª ××¡××›×™×"""
        
        self._ensure_connected()
        collection = self.atlas.get_collection(collection_name)
        
        query = query or {}
        return collection.count_documents(query)
    
    def aggregate_data(self, collection_name: str,
                      pipeline: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """×‘×™×¦×•×¢ ××’×¨×’×¦×™×”"""
        
        self._ensure_connected()
        collection = self.atlas.get_collection(collection_name)
        
        # ××’×¨×’×¦×™×” ××—×–×™×¨×” cursor
        cursor = collection.aggregate(pipeline)
        
        # ×”××¨×” ×œ×¨×©×™××” + × ×™×§×•×™ ObjectId
        results = []
        for doc in cursor:
            if '_id' in doc and doc['_id'] is not None:
                doc['_id'] = str(doc['_id'])
            results.append(doc)
        
        return results
    
    def get_distinct_values(self, collection_name: str,
                           field: str,
                           query: Dict[str, Any] = None) -> List[Any]:
        """×¢×¨×›×™× ×™×™×—×•×“×™×™× ×‘×©×“×”"""
        
        self._ensure_connected()
        collection = self.atlas.get_collection(collection_name)
        
        query = query or {}
        return collection.distinct(field, query)
    
    def disconnect(self):
        """× ×™×ª×•×§"""
        if self.connected:
            self.atlas.disconnect()
            self.connected = False

# ×“×•×’××” ×œ×©×™××•×©:
dal = AtlasDAL(
    "mongodb+srv://username:password@cluster.mongodb.net/",
    "school_db"
)

if dal.connect():
    # ××¦×™××ª ×›×œ ×”×¡×˜×•×“× ×˜×™× ×”×¤×¢×™×œ×™×
    students = dal.find_documents(
        "students",
        query={"active": True},
        sort=[("name", 1)],
        limit=100
    )
    
    print(f"× ××¦××• {len(students)} ×¡×˜×•×“× ×˜×™× ×¤×¢×™×œ×™×")
```

---

## ×§×¨×™××ª × ×ª×•× ×™× - ××ª×™ ×œ×¢×‘×•×¨ ×‘×œ×•×œ××”?

### 1. ×›××•×ª ×§×˜× ×” ×©×œ × ×ª×•× ×™× (×¢×“ 1000 ××¡××›×™×)
```python
# ×˜×•×¢×Ÿ ×”×›×œ ×œ×–×™×›×¨×•×Ÿ ×‘×‘×ª ××—×ª
def load_small_dataset(dal, collection_name, query=None):
    """×œ×˜×¢×™× ×ª ××¢×¨×›×™ × ×ª×•× ×™× ×§×˜× ×™×"""
    
    documents = dal.find_documents(collection_name, query=query)
    
    print(f"× ×˜×¢× ×• {len(documents)} ××¡××›×™× ×œ×–×™×›×¨×•×Ÿ")
    return documents

# ×©×™××•×©:
students = load_small_dataset(dal, "students", {"course": "××ª××˜×™×§×”"})

# ×¢×™×‘×•×“ ×™×©×™×¨
for student in students:
    print(f"{student['name']} - ×¦×™×•×Ÿ: {student.get('grade', 'N/A')}")
```

### 2. ×›××•×ª ×’×“×•×œ×” - ×¢×™×‘×•×“ ×‘×—×œ×§×™× (Chunking)
```python
def process_large_dataset_in_chunks(dal, collection_name, 
                                   query=None, chunk_size=1000):
    """×¢×™×‘×•×“ ××¢×¨×š × ×ª×•× ×™× ×’×“×•×œ ×‘×—×œ×§×™×"""
    
    total_docs = dal.count_documents(collection_name, query)
    print(f"×¡×”\"×› {total_docs} ××¡××›×™× ×œ×¢×™×‘×•×“")
    
    processed = 0
    skip = 0
    
    while skip < total_docs:
        # ×˜×•×¢×Ÿ ×—×œ×§
        chunk = dal.find_documents(
            collection_name,
            query=query,
            skip=skip,
            limit=chunk_size
        )
        
        if not chunk:  # ××™×Ÿ ×™×•×ª×¨ × ×ª×•× ×™×
            break
        
        # ×¢×™×‘×•×“ ×”×—×œ×§
        for doc in chunk:
            # ×›××Ÿ ×ª×¢×©×” ××ª ×”×¢×™×‘×•×“ ×©×œ×š
            process_single_document(doc)
        
        processed += len(chunk)
        skip += chunk_size
        
        print(f"×¢×•×‘×“... {processed}/{total_docs} ({processed/total_docs*100:.1f}%)")
    
    print(f"âœ… ×¡×™×•× ×¢×™×‘×•×“ {processed} ××¡××›×™×")

def process_single_document(doc):
    """×¢×™×‘×•×“ ××¡××š ×™×—×™×“"""
    # ×›××Ÿ ×”×œ×•×’×™×§×” ×©×œ×š ×œ×›×œ ××¡××š
    pass

# ×©×™××•×©:
process_large_dataset_in_chunks(
    dal, 
    "large_collection",
    query={"status": "active"},
    chunk_size=500
)
```

### 3. ×¢×™×‘×•×“ ××¡×™× ×›×¨×•× ×™ (×œ×™×¢×™×œ×•×ª ××§×¡×™××œ×™×ª)
```python
import asyncio
from motor.motor_asyncio import AsyncIOMotorClient

class AsyncAtlasDAL:
    def __init__(self, connection_string: str, database_name: str):
        self.connection_string = connection_string
        self.database_name = database_name
        self.client = None
        self.db = None
    
    async def connect(self):
        self.client = AsyncIOMotorClient(self.connection_string)
        await self.client.admin.command('ping')
        self.db = self.client[self.database_name]
        return True
    
    async def find_documents_async(self, collection_name, query=None, limit=None):
        collection = self.db[collection_name]
        cursor = collection.find(query or {})
        
        if limit:
            cursor = cursor.limit(limit)
        
        documents = []
        async for doc in cursor:  # ×œ×•×œ××” ××¡×™× ×›×¨×•× ×™×ª
            if '_id' in doc:
                doc['_id'] = str(doc['_id'])
            documents.append(doc)
        
        return documents

# ×©×™××•×© ××¡×™× ×›×¨×•× ×™:
async def process_data_async():
    dal = AsyncAtlasDAL(
        "mongodb+srv://username:password@cluster.mongodb.net/",
        "school_db"
    )
    
    await dal.connect()
    
    # ×¢×™×‘×•×“ ××”×™×¨ ×©×œ × ×ª×•× ×™×
    students = await dal.find_documents_async("students", {"active": True})
    
    print(f"× ×˜×¢× ×• {len(students)} ×¡×˜×•×“× ×˜×™× ×‘××”×™×¨×•×ª")
    
    return students

# ×”×¨×¦×”:
students = asyncio.run(process_data_async())
```

---

## ×”××¨×” ×œ-Pandas

### 1. ×”××¨×” ×‘×¡×™×¡×™×ª
```python
def mongo_to_pandas(dal, collection_name, query=None, 
                   flatten_nested=True) -> pd.DataFrame:
    """×”××¨×ª × ×ª×•× ×™ MongoDB ×œ-DataFrame"""
    
    # ×©×œ×™×¤×ª ×”× ×ª×•× ×™×
    documents = dal.find_documents(collection_name, query)
    
    if not documents:
        print("âš ï¸ ×œ× × ××¦××• × ×ª×•× ×™×")
        return pd.DataFrame()
    
    # ×”××¨×” ×œ-DataFrame
    df = pd.DataFrame(documents)
    
    if flatten_nested:
        df = flatten_dataframe(df)
    
    print(f"âœ… × ×•×¦×¨ DataFrame ×¢× {len(df)} ×©×•×¨×•×ª ×•-{len(df.columns)} ×¢××•×“×•×ª")
    print(f"×¢××•×“×•×ª: {list(df.columns)}")
    
    return df

def flatten_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """×¤×™×¨×•×§ ×¢××•×“×•×ª ××§×•× × ×•×ª"""
    
    new_df = df.copy()
    
    for column in df.columns:
        # ×‘×“×™×§×” ×× ×™×© ××•×‘×™×™×§×˜×™× ××§×•× × ×™×
        if df[column].dtype == 'object':
            sample = df[column].dropna().iloc[0] if not df[column].dropna().empty else None
            
            if isinstance(sample, dict):
                # ×¤×™×¨×•×§ dictionary ×œ×¢××•×“×•×ª × ×¤×¨×“×•×ª
                nested_df = pd.json_normalize(df[column])
                nested_df.columns = [f"{column}.{col}" for col in nested_df.columns]
                
                # ×”×¡×¨×ª ×”×¢××•×“×” ×”××§×•×¨×™×ª ×•×”×•×¡×¤×ª ×”×¢××•×“×•×ª ×”×—×“×©×•×ª
                new_df = new_df.drop(columns=[column])
                new_df = pd.concat([new_df, nested_df], axis=1)
    
    return new_df

# ×©×™××•×©:
df_students = mongo_to_pandas(dal, "students", {"course": "××“×¢×™ ×”××—×©×‘"})
print(df_students.head())
```

### 2. ×”××¨×” ××ª×§×“××ª ×¢× × ×™×§×•×™ × ×ª×•× ×™×
```python
def advanced_mongo_to_pandas(dal, collection_name, query=None,
                            date_columns=None, 
                            numeric_columns=None,
                            drop_columns=None) -> pd.DataFrame:
    """×”××¨×” ××ª×§×“××ª ×¢× × ×™×§×•×™ × ×ª×•× ×™×"""
    
    # ×©×œ×™×¤×ª × ×ª×•× ×™×
    documents = dal.find_documents(collection_name, query)
    
    if not documents:
        return pd.DataFrame()
    
    # ×”××¨×” ×‘×¡×™×¡×™×ª
    df = pd.DataFrame(documents)
    
    # ×”×¡×¨×ª ×¢××•×“×•×ª ×œ× ×¨×¦×•×™×•×ª
    if drop_columns:
        df = df.drop(columns=[col for col in drop_columns if col in df.columns])
    
    # ×”××¨×ª ×¢××•×“×•×ª ×ª××¨×™×š
    if date_columns:
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
    
    # ×”××¨×ª ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª
    if numeric_columns:
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # × ×™×§×•×™ ×‘×¡×™×¡×™
    df = df.dropna(how='all')  # ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª ×œ×’××¨×™
    
    print(f"âœ… DataFrame ××•×›×Ÿ: {len(df)} ×©×•×¨×•×ª, {len(df.columns)} ×¢××•×“×•×ª")
    print(f"×¡×•×’×™ × ×ª×•× ×™×:\n{df.dtypes}")
    
    return df

# ×“×•×’××”:
df = advanced_mongo_to_pandas(
    dal, 
    "students",
    query={"active": True},
    date_columns=["enrollment_date", "graduation_date"],
    numeric_columns=["age", "gpa", "credits"],
    drop_columns=["_id", "internal_notes"]
)
```

### 3. ×¢×‘×•×“×” ×¢× ××¢×¨×›×™× ×‘××•× ×’×•
```python
def handle_array_fields(dal, collection_name, array_field, query=None):
    """×˜×™×¤×•×œ × ×›×•×Ÿ ×‘××¢×¨×›×™× ×××•× ×’×•"""
    
    # ×§×•×“× - ×¨××” ××™×š ×”××¢×¨×›×™× × ×¨××™×
    sample = dal.find_one_document(collection_name, query or {})
    if sample and array_field in sample:
        print(f"×“×•×’××” ×œ××¢×¨×š {array_field}: {sample[array_field]}")
    
    documents = dal.find_documents(collection_name, query)
    
    # ×©×ª×™ ××¤×©×¨×•×™×•×ª:
    
    # ××¤×©×¨×•×ª 1: ×”××¢×¨×š ×›×˜×§×¡×˜
    df_as_text = pd.DataFrame(documents)
    df_as_text[f"{array_field}_str"] = df_as_text[array_field].astype(str)
    
    # ××¤×©×¨×•×ª 2: ×¤×™×¨×•×§ ×”××¢×¨×š ×œ×©×•×¨×•×ª × ×¤×¨×“×•×ª
    exploded_data = []
    for doc in documents:
        if array_field in doc and isinstance(doc[array_field], list):
            for item in doc[array_field]:
                new_doc = doc.copy()
                new_doc[f"{array_field}_item"] = item
                exploded_data.append(new_doc)
    
    df_exploded = pd.DataFrame(exploded_data)
    
    return df_as_text, df_exploded

# ×“×•×’××”:
df_grades_text, df_grades_exploded = handle_array_fields(
    dal, "students", "grades", {"course": "××ª××˜×™×§×”"}
)

print("×¢× ××¢×¨×›×™× ×›×˜×§×¡×˜:")
print(df_grades_text[['name', 'grades_str']].head())

print("\n×¢× ××¢×¨×›×™× ××¤×•×¨×§×™×:")
print(df_grades_exploded[['name', 'grades_item']].head())
```

---

## ×“×™×¤×“×•×£ (Pagination) ×‘××¡××›×™×

### 1. ×“×™×¤×“×•×£ ×‘×¡×™×¡×™
```python
def paginate_collection(dal, collection_name, query=None, 
                       page_size=50, max_pages=None):
    """×“×™×¤×“×•×£ ×¢×œ ×›×œ ×”××¡××›×™× ×‘××•×¡×£"""
    
    total_docs = dal.count_documents(collection_name, query)
    total_pages = (total_docs + page_size - 1) // page_size
    
    if max_pages:
        total_pages = min(total_pages, max_pages)
    
    print(f"ğŸ“Š ×¡×”\"×› {total_docs} ××¡××›×™×, {total_pages} ×¢××•×“×™×")
    
    all_data = []
    
    for page in range(total_pages):
        skip = page * page_size
        
        print(f"ğŸ“„ ×˜×•×¢×Ÿ ×¢××•×“ {page + 1}/{total_pages}...")
        
        page_data = dal.find_documents(
            collection_name,
            query=query,
            skip=skip,
            limit=page_size
        )
        
        all_data.extend(page_data)
        
        print(f"   × ×˜×¢× ×• {len(page_data)} ××¡××›×™× (×¡×”\"×›: {len(all_data)})")
        
        # ×”×¤×¡×§×” ×§×˜× ×” ×œ×× ×™×¢×ª ×¢×•××¡
        import time
        time.sleep(0.1)
    
    return all_data

# ×©×™××•×©:
all_students = paginate_collection(
    dal, 
    "students",
    query={"active": True},
    page_size=40,  # 40 ××¡××›×™× ×‘×›×œ ×¢××•×“
    max_pages=10   # ××§×¡×™××•× 10 ×¢××•×“×™×
)

print(f"âœ… × ×˜×¢× ×• ×¡×”\"×› {len(all_students)} ×¡×˜×•×“× ×˜×™×")
```

### 2. ×“×™×¤×“×•×£ ×¢× ×¢×™×‘×•×“ ××™×™×“×™
```python
def process_with_pagination(dal, collection_name, 
                           processor_func, query=None, 
                           page_size=100):
    """×“×™×¤×“×•×£ ×¢× ×¢×™×‘×•×“ ××™×™×“×™ - ×—×•×¡×š ×–×™×›×¨×•×Ÿ"""
    
    total_docs = dal.count_documents(collection_name, query)
    total_pages = (total_docs + page_size - 1) // page_size
    
    print(f"ğŸ”„ ××ª×—×™×œ ×¢×™×‘×•×“ {total_docs} ××¡××›×™× ×‘-{total_pages} ×¢××•×“×™×")
    
    results = []
    
    for page in range(total_pages):
        skip = page * page_size
        
        # ×˜×•×¢×Ÿ ×¢××•×“
        page_data = dal.find_documents(
            collection_name,
            query=query,
            skip=skip,
            limit=page_size
        )
        
        # ××¢×‘×“ ××™×“
        for doc in page_data:
            result = processor_func(doc)
            if result:  # ×¨×§ ×× ×™×© ×ª×•×¦××”
                results.append(result)
        
        print(f"âœ… ×¢××•×“ {page + 1}/{total_pages} - {len(results)} ×ª×•×¦××•×ª ×¢×“ ×›×”")
    
    return results

# ×“×•×’××” ×œ×¤×•× ×§×¦×™×™×ª ×¢×™×‘×•×“
def calculate_student_average(student_doc):
    """×—×™×©×•×‘ ×××•×¦×¢ ×¦×™×•× ×™× ×œ×¡×˜×•×“× ×˜"""
    
    if 'grades' in student_doc and student_doc['grades']:
        grades = [g for g in student_doc['grades'] if isinstance(g, (int, float))]
        
        if grades:
            return {
                'name': student_doc.get('name', 'Unknown'),
                'average': sum(grades) / len(grades),
                'grade_count': len(grades)
            }
    
    return None

# ×©×™××•×©:
averages = process_with_pagination(
    dal,
    "students",
    calculate_student_average,
    query={"grades": {"$exists": True}},
    page_size=50
)

# ×”××¨×” ×œ-Pandas ×œ× ×™×ª×•×— × ×•×¡×£
df_averages = pd.DataFrame(averages)
print(f"×××•×¦×¢ ×›×œ×œ×™: {df_averages['average'].mean():.2f}")
```

### 3. ×“×™×¤×“×•×£ ×¢× MongoDB Cursor (×™×¢×™×œ ×™×•×ª×¨)
```python
def efficient_iteration(dal, collection_name, query=None, batch_size=1000):
    """××™×˜×¨×¦×™×” ×™×¢×™×œ×” ×¢× cursor"""
    
    collection = dal.atlas.get_collection(collection_name)
    cursor = collection.find(query or {}).batch_size(batch_size)
    
    processed = 0
    batch_data = []
    
    try:
        for doc in cursor:
            # × ×§×” ObjectId
            if '_id' in doc:
                doc['_id'] = str(doc['_id'])
            
            batch_data.append(doc)
            processed += 1
            
            # ×›×œ batch_size ××¡××›×™× - ×¢×‘×“ ××•×ª×
            if len(batch_data) >= batch_size:
                yield batch_data.copy()  # ×”×—×–×¨ batch
                batch_data.clear()
                
                print(f"ğŸ”„ ×¢×•×‘×“... {processed} ××¡××›×™×")
        
        # ×”××—×¨×•× ×™× ×©× ×©××¨×•
        if batch_data:
            yield batch_data
    
    finally:
        cursor.close()

# ×©×™××•×©:
for batch in efficient_iteration(dal, "large_collection", batch_size=500):
    # ×¢×™×‘×•×“ ×›×œ batch
    df_batch = pd.DataFrame(batch)
    
    # ×¢×©×” ××©×”×• ×¢× ×”-DataFrame
    print(f"Batch ×¢× {len(df_batch)} ×©×•×¨×•×ª")
    
    # ×“×•×’××”: ×©××™×¨×” ×œ×§×•×‘×¥
    # df_batch.to_csv(f"batch_{len(df_batch)}.csv", index=False)
```

---

## ××“×¨×™×š Pandas ×œ××‘×—×Ÿ

### 1. ×¤×¢×•×œ×•×ª ×‘×¡×™×¡×™×•×ª ×¢×œ DataFrame
```python
# ×§×¨×™××ª ××™×“×¢ ×‘×¡×™×¡×™ ×¢×œ ×”× ×ª×•× ×™×
def explore_dataframe(df):
    """×¡×§×™×¨×” ××”×™×¨×” ×©×œ DataFrame"""
    
    print("ğŸ“Š ××™×“×¢ ×›×œ×œ×™:")
    print(f"   ×’×•×“×œ: {df.shape[0]} ×©×•×¨×•×ª, {df.shape[1]} ×¢××•×“×•×ª")
    print(f"   ×–×™×›×¨×•×Ÿ: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print("\nğŸ“‹ ×¢××•×“×•×ª ×•×¡×•×’×™ × ×ª×•× ×™×:")
    print(df.dtypes)
    
    print("\nğŸ” ×“×•×’×××•×ª:")
    print(df.head(3))
    
    print("\nğŸ“ˆ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×‘×¡×™×¡×™×•×ª:")
    print(df.describe())
    
    print("\nâŒ ×¢×¨×›×™× ×—×¡×¨×™×:")
    missing = df.isnull().sum()
    print(missing[missing > 0])

# ×©×™××•×©:
df = mongo_to_pandas(dal, "students")
explore_dataframe(df)
```

### 2. ×¡×™× ×•×Ÿ ×•×—×™×¤×•×©
```python
# ×¡×™× ×•×Ÿ × ×ª×•× ×™×
def filter_examples(df):
    """×“×•×’×××•×ª ×œ×¡×™× ×•×Ÿ DataFrame"""
    
    # ×¡×™× ×•×Ÿ ×‘×¡×™×¡×™
    active_students = df[df['active'] == True]
    
    # ×¡×™× ×•×Ÿ ××¡×¤×¨×™
    high_gpa = df[df['gpa'] > 3.5]
    
    # ×¡×™× ×•×Ÿ ×˜×§×¡×˜
    cs_students = df[df['course'].str.contains('××“×¢×™', na=False)]
    
    # ×¡×™× ×•×Ÿ ××•×¨×›×‘
    good_cs_students = df[
        (df['course'].str.contains('××“×¢×™', na=False)) & 
        (df['gpa'] > 3.0) &
        (df['active'] == True)
    ]
    
    # ×¡×™× ×•×Ÿ ×¢× ×¨×©×™××”
    specific_courses = df[df['course'].isin(['××ª××˜×™×§×”', '×¤×™×–×™×§×”'])]
    
    print(f"×¡×˜×•×“× ×˜×™× ×¤×¢×™×œ×™×: {len(active_students)}")
    print(f"GPA ×’×‘×•×”: {len(high_gpa)}")
    print(f"××“×¢×™ ×”××—×©×‘: {len(cs_students)}")
    print(f"××“××— ×˜×•×‘×™×: {len(good_cs_students)}")
    
    return good_cs_students

# ×©×™××•×©:
filtered_df = filter_examples(df)
```

### 3. ×§×™×‘×•×¥ ×•×—×™×©×•×‘×™× (GroupBy)
```python
def groupby_examples(df):
    """×“×•×’×××•×ª ×œ×§×™×‘×•×¥ ×•×—×™×©×•×‘×™×"""
    
    # ×§×™×‘×•×¥ ×‘×¡×™×¡×™
    by_course = df.groupby('course').agg({
        'gpa': ['mean', 'std', 'count'],
        'age': ['mean', 'min', 'max'],
        'active': 'sum'  # ×›××” ×¤×¢×™×œ×™×
    }).round(2)
    
    print("ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×œ×¤×™ ×§×•×¨×¡:")
    print(by_course)
    
    # ×§×™×‘×•×¥ ××¨×•×‘×”
    by_course_year = df.groupby(['course', 'year']).agg({
        'gpa': 'mean',
        'student_id': 'count'
    }).round(2)
    
    print("\nğŸ“Š ×œ×¤×™ ×§×•×¨×¡ ×•×©× ×”:")
    print(by_course_year)
    
    # ×—×™×©×•×‘×™× ××•×ª×××™× ××™×©×™×ª
    def custom_stats(group):
        return pd.Series({
            'avg_gpa': group['gpa'].mean(),
            'top_student': group.loc[group['gpa'].idxmax(), 'name'] if not group.empty else 'N/A',
            'pass_rate': (group['gpa'] >= 2.0).sum() / len(group) * 100
        })
    
    custom_by_course = df.groupby('course').apply(custom_stats)
    print("\nğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ××•×ª×××•×ª:")
    print(custom_by_course)
    
    return by_course, custom_by_course

# ×©×™××•×©:
stats, custom_stats = groupby_examples(df)
```

### 4. ×¢×‘×•×“×” ×¢× ×ª××¨×™×›×™×
```python
def datetime_operations(df):
    """×¤×¢×•×œ×•×ª ×¢×œ ×ª××¨×™×›×™×"""
    
    # ×”××¨×” ×œ×ª××¨×™×š (×× ×¢×“×™×™×Ÿ ×œ×)
    if 'enrollment_date' in df.columns:
        df['enrollment_date'] = pd.to_datetime(df['enrollment_date'])
        
        # ×—×™×œ×•×¥ ××¨×›×™×‘×™ ×ª××¨×™×š
        df['enrollment_year'] = df['enrollment_date'].dt.year
        df['enrollment_month'] = df['enrollment_date'].dt.month
        df['enrollment_day_of_week'] = df['enrollment_date'].dt.day_name()
        
        # ×—×™×©×•×‘×™ ×–××Ÿ
        df['days_since_enrollment'] = (pd.Timestamp.now() - df['enrollment_date']).dt.days
        df['years_since_enrollment'] = df['days_since_enrollment'] / 365.25
        
        # ×§×™×‘×•×¥ ×œ×¤×™ ×ª×§×•×¤×•×ª
        enrollment_by_year = df.groupby('enrollment_year').size()
        enrollment_by_month = df.groupby(['enrollment_year', 'enrollment_month']).size()
        
        print("ğŸ“… ×¨×™×©×•× ×œ×¤×™ ×©× ×™×:")
        print(enrollment_by_year)
        
        return df
    else:
        print("âš ï¸ ××™×Ÿ ×¢××•×“×ª ×ª××¨×™×š")
        return df

# ×©×™××•×©:
df = datetime_operations(df)
```

### 5. ×¢×‘×•×“×” ×¢× ××¢×¨×›×™× ×-MongoDB
```python
def handle_mongodb_arrays(df):
    """×˜×™×¤×•×œ ×‘××¢×¨×›×™× ×©×”×’×™×¢×• ×-MongoDB"""
    
    if 'grades' in df.columns:
        # ×“×¨×š 1: ×—×™×©×•×‘ ×××•×¦×¢ ×œ××¢×¨×š ×‘×›×œ ×©×•×¨×”
        df['avg_grade'] = df['grades'].apply(
            lambda x: sum(x) / len(x) if isinstance(x, list) and x else None
        )
        
        # ×“×¨×š 2: ×›××•×ª ×”×¦×™×•× ×™×
        df['grade_count'] = df['grades'].apply(
            lambda x: len(x) if isinstance(x, list) else 0
        )
        
        # ×“×¨×š 3: ×”×¦×™×•×Ÿ ×”×’×‘×•×” ×‘×™×•×ª×¨
        df['max_grade'] = df['grades'].apply(
            lambda x: max(x) if isinstance(x, list) and x else None
        )
        
        # ×“×¨×š 4: ×¤×™×¨×•×§ ×”××¢×¨×š ×œ×¢××•×“×•×ª × ×¤×¨×“×•×ª (×× ×™×© ××¡×¤×¨ ×§×‘×•×¢)
        max_grades = df['grades'].apply(lambda x: len(x) if isinstance(x, list) else 0).max()
        if max_grades and max_grades <= 10:  # ×¨×§ ×× ×œ× ×™×•×ª×¨ ×-10 ×¦×™×•× ×™×
            for i in range(max_grades):
                df[f'grade_{i+1}'] = df['grades'].apply(
                    lambda x: x[i] if isinstance(x, list) and len(x) > i else None
                )
        
        print("âœ… ×¢××•×“×•×ª ×—×“×©×•×ª × ×•×¦×¨×•:")
        print(f"   avg_grade, grade_count, max_grade")
        if max_grades <= 10:
            print(f"   grade_1 ×¢×“ grade_{max_grades}")
    
    return df

# ×©×™××•×©:
df = handle_mongodb_arrays(df)
```

### 6. ××™×•×Ÿ ×•×“×™×¨×•×’
```python
def sorting_and_ranking(df):
    """××™×•×Ÿ ×•×“×™×¨×•×’ × ×ª×•× ×™×"""
    
    # ××™×•×Ÿ ×‘×¡×™×¡×™
    top_students = df.nlargest(10, 'gpa')
    bottom_students = df.nsmallest(10, 'gpa')
    
    # ××™×•×Ÿ ××•×¨×›×‘
    sorted_df = df.sort_values(['course', 'gpa'], ascending=[True, False])
    
    # ×“×™×¨×•×’
    df['gpa_rank'] = df['gpa'].rank(ascending=False, method='min')
    df['gpa_rank_by_course'] = df.groupby('course')['gpa'].rank(ascending=False, method='min')
    
    # ××—×•×–×•× ×™×
    df['gpa_percentile'] = df['gpa'].rank(pct=True) * 100
    
    # ×§×˜×’×•×¨×™×•×ª ×‘×™×¦×•×¢×™×
    df['performance_category'] = pd.cut(
        df['gpa'], 
        bins=[0, 2.0, 3.0, 3.5, 4.0], 
        labels=['× ××•×š', '×‘×™× ×•× ×™', '×˜×•×‘', '××¢×•×œ×”']
    )
    
    print("ğŸ† 10 ×”×¡×˜×•×“× ×˜×™× ×”×˜×•×‘×™×:")
    print(top_students[['name', 'course', 'gpa']].head())
    
    print(f"\nğŸ“Š ×”×ª×¤×œ×’×•×ª ×‘×™×¦×•×¢×™×:")
    print(df['performance_category'].value_counts())
    
    return df

# ×©×™××•×©:
df = sorting_and_ranking(df)
```

### 7. ×™×¦×™×¨×ª ×“×•×—×•×ª ××”×™×¨×™×
```python
def create_quick_reports(df):
    """×™×¦×™×¨×ª ×“×•×—×•×ª ××”×™×¨×™× ×œ××‘×—×Ÿ"""
    
    print("=" * 60)
    print("ğŸ“‹ ×“×•×— ××”×™×¨ - ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×˜×•×“× ×˜×™×")
    print("=" * 60)
    
    # 1. ×¡×™×›×•× ×›×œ×œ×™
    print(f"\nğŸ“Š ×¡×™×›×•× ×›×œ×œ×™:")
    print(f"   ×¡×”\"×› ×¡×˜×•×“× ×˜×™×: {len(df):,}")
    print(f"   ×¡×˜×•×“× ×˜×™× ×¤×¢×™×œ×™×: {df['active'].sum():,}")
    print(f"   ×××•×¦×¢ GPA ×›×œ×œ×™: {df['gpa'].mean():.2f}")
    print(f"   ×’×™×œ ×××•×¦×¢: {df['age'].mean():.1f}")
    
    # 2. ×œ×¤×™ ×§×•×¨×¡×™×
    print(f"\nğŸ“š ×œ×¤×™ ×§×•×¨×¡×™×:")
    course_stats = df.groupby('course').agg({
        'student_id': 'count',
        'gpa': 'mean',
        'active': 'sum'
    }).round(2)
    course_stats.columns = ['×¡×˜×•×“× ×˜×™×', '×××•×¦×¢_GPA', '×¤×¢×™×œ×™×']
    print(course_stats)
    
    # 3. ×”×ª×¤×œ×’×•×ª ×’×™×œ××™×
    print(f"\nğŸ‘¥ ×”×ª×¤×œ×’×•×ª ×’×™×œ××™×:")
    age_bins = pd.cut(df['age'], bins=[0, 20, 25, 30, 100], labels=['×¢×“ 20', '21-25', '26-30', '30+'])
    print(age_bins.value_counts())
    
    # 4. ×”×™×©×’×™×
    print(f"\nğŸ¯ ×”×™×©×’×™×:")
    print(f"   GPA ××¢×œ 3.5: {(df['gpa'] > 3.5).sum()} ×¡×˜×•×“× ×˜×™×")
    print(f"   GPA ××ª×—×ª ×œ-2.0: {(df['gpa'] < 2.0).sum()} ×¡×˜×•×“× ×˜×™×")
    
    # 5. ×¡×˜×•×“× ×˜×™× ××•×‘×™×œ×™×
    print(f"\nğŸ† 5 ×”×¡×˜×•×“× ×˜×™× ×”××•×‘×™×œ×™×:")
    top_5 = df.nlargest(5, 'gpa')[['name', 'course', 'gpa']]
    for idx, row in top_5.iterrows():
        print(f"   {row['name']} ({row['course']}) - GPA: {row['gpa']}")
    
    print("=" * 60)

# ×©×™××•×©:
create_quick_reports(df)
```

---

## ×˜×™×¤×™× ×œ××‘×—×Ÿ

### 1. ×–×¨×™××ª ×¢×‘×•×“×” ××•××œ×¦×ª
```python
def exam_workflow(connection_string, database_name, collection_name):
    """×–×¨×™××ª ×¢×‘×•×“×” ××œ××” ×œ××‘×—×Ÿ"""
    
    print("ğŸš€ ××ª×—×™×œ ×–×¨×™××ª ×¢×‘×•×“×” ×œ××‘×—×Ÿ...")
    
    # ×©×œ×‘ 1: ×—×™×‘×•×¨
    dal = AtlasDAL(connection_string, database_name)
    if not dal.connect():
        print("âŒ ×—×™×‘×•×¨ × ×›×©×œ!")
        return None
    
    # ×©×œ×‘ 2: ×‘×“×™×§×” ×¨××©×•× ×™×ª
    print("\nğŸ” ×‘×“×™×§×” ×¨××©×•× ×™×ª:")
    sample = dal.find_one_document(collection_name, {})
    if sample:
        print(f"   ××‘× ×” ××¡××š: {list(sample.keys())}")
        print(f"   ×“×•×’××”: {sample}")
    
    total_docs = dal.count_documents(collection_name)
    print(f"   ×¡×”\"×› ××¡××›×™×: {total_docs:,}")
    
    # ×©×œ×‘ 3: ×˜×¢×™× ×” ×—×›××”
    if total_docs > 10000:
        print("\nğŸ“¦ ××¢×¨×š × ×ª×•× ×™× ×’×“×•×œ - ×˜×•×¢×Ÿ ×‘×× ×•×ª...")
        all_data = paginate_collection(dal, collection_name, page_size=1000, max_pages=10)
    else:
        print("\nğŸ“¦ ×˜×•×¢×Ÿ ×›×œ ×”× ×ª×•× ×™×...")
        all_data = dal.find_documents(collection_name)
    
    # ×©×œ×‘ 4: ×”××¨×” ×œ-Pandas
    print(f"\nğŸ¼ ×™×•×¦×¨ DataFrame...")
    df = pd.DataFrame(all_data)
    
    # × ×™×§×•×™ ××•×˜×•××˜×™
    df = df.dropna(how='all')  # ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª
    
    # ×–×™×”×•×™ ×¢××•×“×•×ª ×ª××¨×™×š
    date_columns = []
    for col in df.columns:
        if 'date' in col.lower() or 'time' in col.lower():
            try:
                df[col] = pd.to_datetime(df[col])
                date_columns.append(col)
            except:
                pass
    
    if date_columns:
        print(f"   ×–×•×”×• ×¢××•×“×•×ª ×ª××¨×™×š: {date_columns}")
    
    # ×©×œ×‘ 5: ×¡×§×™×¨×” ××”×™×¨×”
    print(f"\nğŸ“Š DataFrame ××•×›×Ÿ:")
    print(f"   ×’×•×“×œ: {df.shape[0]:,} ×©×•×¨×•×ª, {df.shape[1]} ×¢××•×“×•×ª")
    print(f"   ×¢××•×“×•×ª: {list(df.columns)}")
    
    # × ×™×ª×•×§
    dal.disconnect()
    
    return df

# ×©×™××•×© ×‘××‘×—×Ÿ:
df = exam_workflow(
    "mongodb+srv://username:password@cluster.mongodb.net/",
    "exam_database",
    "exam_collection"
)

if df is not None:
    # ×”××©×š ×¢× ×”× ×™×ª×•×—...
    explore_dataframe(df)
    create_quick_reports(df)
```

### 2. ×¤×•× ×§×¦×™×•×ª ××”×™×¨×•×ª ×œ××‘×—×Ÿ
```python
# ××¢×¨×š ×¤×•× ×§×¦×™×•×ª ××•×¢×™×œ×•×ª ×œ××‘×—×Ÿ
def quick_mongo_analysis(dal, collection_name):
    """× ×™×ª×•×— ××”×™×¨ ×©×œ ×§×•×œ×§×©×Ÿ MongoDB"""
    
    # 1. ××™×“×¢ ×›×œ×œ×™
    total = dal.count_documents(collection_name)
    sample = dal.find_one_document(collection_name, {})
    
    print(f"ğŸ“Š {collection_name}:")
    print(f"   ××¡××›×™×: {total:,}")
    print(f"   ×©×“×•×ª: {list(sample.keys()) if sample else 'N/A'}")
    
    # 2. ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×‘×©×“×•×ª ××¤×ª×—
    key_fields = ['status', 'type', 'category', 'active']
    for field in key_fields:
        if sample and field in sample:
            unique_vals = dal.get_distinct_values(collection_name, field)
            print(f"   {field}: {unique_vals}")
    
    return sample, total

def quick_pandas_summary(df, target_column=None):
    """×¡×™×›×•× ××”×™×¨ ×©×œ DataFrame"""
    
    print(f"ğŸ“ˆ ×¡×™×›×•× DataFrame:")
    print(f"   ×’×•×“×œ: {df.shape}")
    print(f"   ×¡×•×’×™ × ×ª×•× ×™×: {df.dtypes.value_counts().to_dict()}")
    
    if target_column and target_column in df.columns:
        print(f"\nğŸ¯ × ×™×ª×•×— {target_column}:")
        if df[target_column].dtype in ['int64', 'float64']:
            print(f"   ×××•×¦×¢: {df[target_column].mean():.2f}")
            print(f"   ×—×¦×™×•×Ÿ: {df[target_column].median():.2f}")
            print(f"   ×˜×•×•×—: {df[target_column].min():.2f} - {df[target_column].max():.2f}")
        else:
            print(f"   ×¢×¨×›×™× ×™×™×—×•×“×™×™×: {df[target_column].nunique()}")
            print(f"   ×”×©×›×™×—×™×: {df[target_column].value_counts().head(3).to_dict()}")

# ×“×•×’××” ×œ××‘×—×Ÿ:
sample, total = quick_mongo_analysis(dal, "students")
df = mongo_to_pandas(dal, "students")
quick_pandas_summary(df, "gpa")
```

### 3. ×ª×‘× ×™×•×ª ×©××™×œ×ª×•×ª × ×¤×•×¦×•×ª ×œ××‘×—×Ÿ
```python
# ×ª×‘× ×™×•×ª ××•×›× ×•×ª ×œ×©××™×œ×ª×•×ª × ×¤×•×¦×•×ª
COMMON_QUERIES = {
    "active_records": {"active": True},
    "recent_month": {"created_date": {"$gte": "2024-01-01"}},
    "high_value": {"amount": {"$gte": 1000}},
    "has_email": {"email": {"$exists": True}},
    "specific_categories": {"category": {"$in": ["A", "B", "C"]}},
    "text_search": {"name": {"$regex": "search_term", "$options": "i"}}
}

def run_common_query(dal, collection_name, query_name, **params):
    """×”×¨×¦×ª ×©××™×œ×ª×” × ×¤×•×¦×” ×¢× ×¤×¨××˜×¨×™×"""
    
    query = COMMON_QUERIES.get(query_name, {}).copy()
    
    # ×”×ª×××ª ×¤×¨××˜×¨×™×
    for key, value in params.items():
        if key in query:
            query[key] = value
    
    results = dal.find_documents(collection_name, query)
    print(f"ğŸ” {query_name}: {len(results)} ×ª×•×¦××•×ª")
    
    return results

# ×©×™××•×©:
active_students = run_common_query(dal, "students", "active_records")
high_grades = run_common_query(dal, "students", "high_value", amount=85)  # ××ª××™× ×œ-grade
```

---

## ×œ×¡×™×›×•× - ×”×¨×©×™××” ×”×—×©×•×‘×” ×œ××‘×—×Ÿ

### âœ… ×¡×“×¨ ×¤×¢×•×œ×•×ª ×‘××‘×—×Ÿ:
1. **×—×™×‘×•×¨ ×œ-Atlas** â†’ `AtlasDAL`
2. **×‘×“×™×§×” ×¨××©×•× ×™×ª** â†’ `find_one_document()`
3. **×˜×¢×™× ×ª × ×ª×•× ×™×** â†’ `find_documents()` ××• `paginate_collection()`
4. **×”××¨×” ×œ-Pandas** â†’ `pd.DataFrame()`
5. **× ×™×ª×•×— ×•×¢×™×‘×•×“** â†’ ×¤×¢×•×œ×•×ª Pandas
6. **× ×™×ª×•×§** â†’ `disconnect()`

### ğŸ”§ ×¤×•× ×§×¦×™×•×ª ×—×•×‘×” ×œ×–×›×•×¨:
```python
# MongoDB
dal.find_documents()          # ×˜×¢×™× ×ª ××¡××›×™×
dal.find_one_document()       # ××¡××š ××—×“
dal.count_documents()         # ×¡×¤×™×¨×”
dal.aggregate_data()          # ××’×¨×’×¦×™×”

# Pandas
df.groupby().agg()           # ×§×™×‘×•×¥ ×•×—×™×©×•×‘
df[df['column'] > value]     # ×¡×™× ×•×Ÿ
df.sort_values()             # ××™×•×Ÿ
df.describe()                # ×¡×˜×˜×™×¡×˜×™×§×•×ª
```

### âš ï¸ ×©×’×™××•×ª ×œ×”×™×× ×¢ ××”×Ÿ:
1. ×œ× ×œ×‘×“×•×§ ××ª ××‘× ×” ×”× ×ª×•× ×™× ×§×•×“×
2. ×œ×˜×¢×•×Ÿ ×™×•×ª×¨ ××“×™ × ×ª×•× ×™× ×‘×‘×ª ××—×ª
3. ×œ×©×›×•×— ×œ× ×§×•×ª ObjectId
4. ×œ× ×œ×˜×¤×œ ×‘×¢×¨×›×™× ×—×¡×¨×™×
5. ×œ×©×›×•×— × ×™×ª×•×§ ××”××¡×“

### ğŸš€ ×§×•×“ ××™× ×™××œ×™ ×œ××‘×—×Ÿ:
```python
# ×”×ª×—×œ×” ××”×™×¨×”
dal = AtlasDAL("connection_string", "db_name")
dal.connect()

# ×‘×“×™×§×”
sample = dal.find_one_document("collection", {})
print(sample)

# ×˜×¢×™× ×”
data = dal.find_documents("collection", {"active": True})
df = pd.DataFrame(data)

# × ×™×ª×•×— ×‘×¡×™×¡×™
print(df.describe())
print(df.groupby('category').size())

dal.disconnect()
```

×¢× ×”××“×¨×™×š ×”×–×” ××ª×” ××•×›×Ÿ ×œ×›×œ ×ª×¨×—×™×© ×‘××‘×—×Ÿ! ğŸ¯