### **פרומפט: בניית מנוע אנליטיקות מודולרי עם FastAPI, MongoDB Atlas ו-Pandas**

**הקדמה:**
אתה מפתח Python בכיר וארכיטקט תוכנה. משימתך היא לתכנן ולבנות שירות מיקרו (microservice) חזק, גמיש ומוכן לפרודקשן, שישמש **כמנוע אנליטיקות** על פי דרישה. השירות יתחבר למסד נתונים קיים ב-MongoDB Atlas, יבצע שאילתות מורכבות ודינמיות, ויפעיל **פייפליין עיבוד מודולרי** על הנתונים. התוצאות המעובדות ייחשפו דרך API מודרני מבוסס FastAPI. הפרויקט כולו צריך להיות ארוז ב-Docker ומוכן לפריסה בסביבת OpenShift/Kubernetes.

---

#### **שלב 1: תכנון ומימוש שכבת גישה לנתונים (DAL) עם Query Builder**

זוהי ליבת המערכת. עליך לבנות DAL שיסתיר את המורכבות של MongoDB ויספק ממשק פייתוני נקי לבניית שאילתות.

**דרישות טכניות:**
1.  **חיבור ל-MongoDB Atlas:** ה-DAL יאותחל עם Connection String מאובטח וינהל את החיבור לשירות הענן.
2.  **מימוש `MongoQueryBuilder`:** יש לממש קלאס שישתמש ב-**Builder Pattern** ויספק **ממשק זורם (Fluent API)** לבניית שאילתות. על ה-Builder לתמוך ביכולות הבאות:

    *   **סינון (`add_filter`):**
        *   **אופרטורים בסיסיים:** `eq` (שווה), `ne` (לא שווה), `gt` (גדול מ-), `gte` (גדול/שווה), `lt` (קטן מ-), `lte` (קטן/שווה).
        *   **אופרטורים של רשימה:** `in` (נמצא בתוך רשימה), `nin` (לא נמצא ברשימה).
        *   **אופרטורים מבניים:** `exists` (האם שדה קיים), `type` (בדיקת סוג השדה).
        *   **חיפוש טקסט:** `regex` (ביטויים רגולריים), עם תמיכה באופציות (כמו `case-insensitive`).

    *   **שאילתות על מערכים (Arrays):**
        *   **`add_filter(field, "all", [val1, val2])`**: לבדיקה האם מערך מכיל את כל הערכים.
        *   **`add_filter(field, "size", count)`**: לבדיקת גודל המערך.
        *   **`add_elem_match(field, criteria_dict)`**: לבניית שאילתות `$elemMatch` מורכבות על מסמכים בתוך מערך.

    *   **לוגיקה מורכבת:**
        *   **`AND`**: יתמך באופן מובנה על ידי שרשור של מספר קריאות `add_filter`.
        *   **`add_or_condition([...])`**: מתודה ייעודית לבניית שאילתות `$or`.

    *   **עיצוב התוצאה:**
        *   **`set_sort(field, direction)`**: למיון התוצאות.
        *   **`set_limit(count)`**: להגבלת מספר התוצאות.
        *   **`set_skip(count)`**: לדילוג על תוצאות (לצורך דיפדוף).

    *   **ביצוע:**
        *   **`execute()`**: מתודה אסינכרונית שתבנה את השאילתה המלאה, תריץ אותה מול MongoDB, ותחזיר את התוצאות כרשימה של מילונים.

---

#### **שלב 2: בניית שכבת API עם FastAPI**

עליך לבנות את הממשק החיצוני לשירות. ה-API יצטרך לקבל לא רק פרמטרים לסינון הנתונים הראשוני, אלא גם רשימה של הניתוחים והאגרגציות הספציפיות שהמשתמש רוצה להפעיל.

**דרישות טכניות:**
1.  **קלט מבוסס Pydantic:** יש להגדיר מודל `QueryRequest` שיכיל שני חלקים עיקריים:
    *   `criteria`: אובייקט `SearchCriteria` שיגדיר את תנאי הסינון, המיון והדיפדוף של הנתונים הגולמיים.
    *   `analyses`: **רשימה של מחרוזות** (למשל, `["sales_by_region", "user_activity_summary"]`), המציינת אילו "שירותי עיבוד" המשתמש מבקש להריץ.
2.  **נקודת קצה ראשית:** `POST /api/analyze`. נקודת הקצה תקבל את אובייקט ה-`QueryRequest`.
3.  **לוגיקת ה-Endpoint:** הלוגיקה תהיה אחראית על:
    *   שימוש ב-DAL ובפרמטר ה-`criteria` כדי לשלוף את הנתונים הגולמיים מ-MongoDB.
    *   העברת הנתונים הגולמיים ו**רשימת הניתוחים** המבוקשים (`analyses`) לשלב העיבוד.
    *   החזרת התוצאות המעובדות, במבנה של מילון שבו כל מפתח הוא שם הניתוח והערך הוא התוצאה שלו.
4.  **בדיקות תקינות (Health Checks):** יש לממש שני endpoints:
    *   `GET /`: בדיקת liveness בסיסית.
    *   `GET /health`: בדיקת readiness מפורטת, שמוודאת גם את תקינות החיבור ל-MongoDB Atlas.

---

#### **שלב 3: תכנון ומימוש שכבת עיבוד נתונים מודולרית (Processing Layer)**

זהו החלק המרכזי והייחודי של השירות. המטרה היא לבנות מערך מודולרי של **"שירותי עיבוד" (Processing Services)**, כאשר כל אחד מהם אחראי על ביצוע אגרגציות וניתוחים ספציפיים. יש לתכנן את המערכת כך שתתמוך בשתי גישות עיבוד:

**ארכיטקטורה משותפת לשתי הגישות:**
*   יש להגדיר קלאס בסיס אבסטרקטי, `AbstractAnalysisService`.
*   כל לוגיקת אגרגציה ספציפית תמומש **בקלאס נפרד** שיורש מקלאס הבסיס (למשל, `SalesByRegionService`, `UserActivitySummaryService`).
*   יש לממש "מנהל פייפליין" (Pipeline Manager) שיקבל את הנתונים ואת רשימת הניתוחים המבוקשים, יפעיל את שירותי העיבוד הרלוונטיים, ויאסוף את תוצאותיהם.

---

##### **גישה א': עיבוד מלא בצד האפליקציה (Pandas-Centric)**
בגישה זו, כל הנתונים הגולמיים (לאחר סינון ראשוני) נשלפים מה-DAL, והעיבוד הכבד מתבצע בזיכרון של האפליקציה באמצעות Pandas. זוהי גישה פשוטה יותר למימוש ראשוני, המתאימה למערכי נתונים קטנים עד בינוניים.

**דרישות טכניות לגישה א':**
1.  **שליפת נתונים גולמיים:** ה-DAL ישתמש בפקודת `find()` (באמצעות ה-Query Builder) כדי למשוך את כל המסמכים שעונים על הקריטריונים.
2.  **מימוש שירותי העיבוד:** כל קלאס שירות (למשל `SalesByRegionService`) יקבל DataFrame של Pandas כקלט.
3.  **לוגיקת אגרגציה ב-Pandas:** בתוך כל שירות, יש לממש את הלוגיקה באמצעות יכולות Pandas, כגון:
    *   שימוש נרחב ב-`.groupby()` בשילוב עם `.agg()` לביצוע מספר אגרגציות במקביל (למשל, `sum`, `mean`, `count`).
    *   יצירת עמודות מחושבות הנדרשות לניתוח.
4.  **פלט:** כל שירות יחזיר מבנה נתונים מסכם (למשל, DataFrame קטן יותר או מילון).

---

##### **גישה ב': עיבוד היברידי עם אופטימיזציה (Database-First)**
זוהי הגישה המקצועית והמומלצת, המתאימה למערכי נתונים גדולים. בגישה זו, אנו "דוחפים" את עיקר עבודת האגרגציה הכבדה אל מסד הנתונים עצמו באמצעות ה-**Aggregation Framework** של MongoDB. האפליקציה מקבלת תוצאה קטנה ומסוכמת מראש, ועליה מבצעת עיבודים סופיים קלים יותר עם Pandas.

**דרישות טכניות לגישה ב':**
1.  **הרחבת ה-DAL:** יש להוסיף ל-DAL מתודה ייעודית, למשל `execute_aggregation_pipeline(pipeline_stages: list) -> list`, שתדע להריץ פייפליין אגרגציה מוגדר מראש.
2.  **מימוש שירותי העיבוד (גרסה מתקדמת):**
    *   כל קלאס שירות (למשל `SalesByRegionService`) יהיה אחראי על **בניית פייפליין האגרגציה** הספציפי שלו. הפייפליין יכלול שלבים כמו `$match`, `$group`, `$sort`.
    *   השירות יקרא למתודת ה-`execute_aggregation_pipeline` של ה-DAL ויקבל ממנה תוצאה מסוכמת.
3.  **עיבוד קל ב-Pandas:**
    *   את התוצאה המסוכמת (שהיא כבר קטנה ויעילה) ניתן יהיה לטעון ל-DataFrame של Pandas.
    *   בשלב זה, נבצע רק עיבודים קלים שקשה לבצע ב-MongoDB, כמו חישובים מורכבים ספציפיים, שינוי שמות עמודות, או עיצוב הפלט הסופי.

---

#### **שלב 4: אריזה ופריסה (Containerization & Deployment)**

עליך להכין את השירות לפריסה בסביבת Production מבוססת קונטיינרים.

**דרישות טכניות:**
1.  **`Dockerfile`:** יש לכתוב `Dockerfile` יעיל, שיכלול:
    *   שימוש ב-base image רשמי ורזה של Python.
    *   יצירת משתמש שאינו `root`.
    *   התקנת תלויות מקובץ `requirements.txt`.
    *   העתקת קוד המקור.
    *   הגדרת פקודת `CMD` להרצת השירות עם `uvicorn`.
2.  **מניפסטים של Kubernetes/OpenShift:** יש ליצור סט קבצי YAML להרצת האפליקציה:
    *   **`Secret`**: לאחסון מאובטח של ה-Connection String של MongoDB Atlas.
    *   **`Deployment`**: להגדרת הרצת הקונטיינר, כולל קישור ל-Secret, הגדרת משאבי CPU/Memory, ובדיקות liveness ו-readiness.
    *   **`Service`**: לחשיפת ה-Deployment בתוך הקלאסטר.
    *   **`Route` / `Ingress`**: לחשיפת השירות לעולם החיצון.

כל התעוד עליו להיות רק בעברית בגלל שהוא נועד שאוכל להביו ולדעת מה להעתיק במידת הצורך
ולכן יש בנפרד מזה לצרף מדריכים והסברים על כל דבר שיסבירו את כל התהליך
ואיך להשתמש 
מה הפרמטרים עבור כל דבר וכד'
אבל חלק זה אחרי המערכת
כלומר קודם לבנות עד פה

ורק אחרי זה שאני יאשר שהכל תקין
תעבור לחצי של המדריכים