# ××“×¨×™×š ××ª×§×“×: ××’×¨×’×¦×™×•×ª MongoDB + × ×™×ª×•×— Pandas

## ××™×š ×œ×§×‘×•×¢ ××ª×™ ×œ×”×©×ª××© ×‘×›×œ ×›×œ×™?

### ×›×œ×œ ×”×–×”×‘:
- **MongoDB Aggregation** â†’ ×›×©××ª×” ×¨×•×¦×” ×œ×¦××¦× ××ª ×›××•×ª ×”× ×ª×•× ×™× ×œ×¤× ×™ ×©×œ×™×¤×”
- **Pandas** â†’ ×›×©××ª×” ×¨×•×¦×” ×œ× ×ª×— ×•×œ×¢×‘×“ × ×ª×•× ×™× ×©×›×‘×¨ × ×©×œ×¤×•

---

## ××’×¨×’×¦×™×•×ª MongoDB ××ª×§×“××•×ª

### 1. ×”××‘× ×” ×”×¡×˜× ×“×¨×˜×™ ×©×œ ×¤×™×™×¤×œ×™×™×Ÿ ××’×¨×’×¦×™×”

```python
def build_aggregation_pipeline(match_conditions=None, 
                              group_by_field=None,
                              calculations=None,
                              sort_field=None,
                              limit_count=None):
    """×‘×•× ×” ×¤×™×™×¤×œ×™×™×Ÿ ××’×¨×’×¦×™×” ×¡×˜× ×“×¨×˜×™"""
    
    pipeline = []
    
    # ×©×œ×‘ 1: ×¡×™× ×•×Ÿ (×ª××™×“ ×¨××©×•×Ÿ!)
    if match_conditions:
        pipeline.append({"$match": match_conditions})
    
    # ×©×œ×‘ 2: ×§×™×‘×•×¥ ×•×—×™×©×•×‘
    if group_by_field and calculations:
        group_stage = {"_id": f"${group_by_field}"}
        group_stage.update(calculations)
        pipeline.append({"$group": group_stage})
    
    # ×©×œ×‘ 3: ××™×•×Ÿ
    if sort_field:
        sort_direction = -1 if sort_field.startswith('-') else 1
        field_name = sort_field.lstrip('-')
        pipeline.append({"$sort": {field_name: sort_direction}})
    
    # ×©×œ×‘ 4: ×”×’×‘×œ×” (×ª××™×“ ××—×¨×•×Ÿ!)
    if limit_count:
        pipeline.append({"$limit": limit_count})
    
    return pipeline

# ×“×•×’××” ×œ×©×™××•×©:
pipeline = build_aggregation_pipeline(
    match_conditions={"active": True, "year": {"$gte": 2023}},
    group_by_field="course",
    calculations={
        "student_count": {"$sum": 1},
        "avg_gpa": {"$avg": "$gpa"},
        "max_gpa": {"$max": "$gpa"}
    },
    sort_field="-student_count",  # ××™×•×Ÿ ×™×•×¨×“
    limit_count=10
)

results = dal.aggregate_data("students", pipeline)
```

### 2. ××’×¨×’×¦×™×•×ª ××•×¨×›×‘×•×ª ×œ×ª×¨×—×™×©×™ ××‘×—×Ÿ

#### ×ª×¨×—×™×©: × ×™×ª×•×— ××›×™×¨×•×ª ×œ×¤×™ ×—×•×“×© ×•××–×•×¨
```python
def sales_analysis_by_month_region(dal):
    """× ×™×ª×•×— ××›×™×¨×•×ª ××ª×§×“×"""
    
    pipeline = [
        # ×©×œ×‘ 1: ×¡×™× ×•×Ÿ ×œ×©× ×” ×”××—×¨×•× ×”
        {
            "$match": {
                "sale_date": {"$gte": "2023-01-01"},
                "status": "completed"
            }
        },
        
        # ×©×œ×‘ 2: ×”×•×¡×¤×ª ×©×“×•×ª ××—×•×©×‘×™×
        {
            "$addFields": {
                "year_month": {
                    "$dateToString": {
                        "format": "%Y-%m",
                        "date": {"$dateFromString": {"dateString": "$sale_date"}}
                    }
                },
                "quarter": {
                    "$concat": [
                        {"$toString": {"$year": {"$dateFromString": {"dateString": "$sale_date"}}}},
                        "-Q",
                        {"$toString": {
                            "$ceil": {
                                "$divide": [
                                    {"$month": {"$dateFromString": {"dateString": "$sale_date"}}},
                                    3
                                ]
                            }
                        }}
                    ]
                }
            }
        },
        
        # ×©×œ×‘ 3: ×§×™×‘×•×¥ ××¨×•×‘×” - ×œ×¤×™ ××–×•×¨ ×•×—×•×“×©
        {
            "$group": {
                "_id": {
                    "region": "$region",
                    "year_month": "$year_month",
                    "quarter": "$quarter"
                },
                "total_sales": {"$sum": "$amount"},
                "transaction_count": {"$sum": 1},
                "avg_transaction": {"$avg": "$amount"},
                "unique_customers": {"$addToSet": "$customer_id"}
            }
        },
        
        # ×©×œ×‘ 4: ×”×•×¡×¤×ª ×—×™×©×•×‘×™× × ×•×¡×¤×™×
        {
            "$addFields": {
                "customer_count": {"$size": "$unique_customers"}
            }
        },
        
        # ×©×œ×‘ 5: ××™×•×Ÿ ×œ×¤×™ ××–×•×¨ ×•××– ×œ×¤×™ ×—×•×“×©
        {
            "$sort": {
                "_id.region": 1,
                "_id.year_month": 1
            }
        }
    ]
    
    results = dal.aggregate_data("sales", pipeline)
    
    # ×”××¨×” ×œ-DataFrame ×œ× ×™×ª×•×— × ×•×¡×£
    df = pd.DataFrame(results)
    
    if not df.empty:
        # ×¤×™×¨×•×§ ×”×©×“×” ×”××§×•×‘×¥
        df['region'] = df['_id'].apply(lambda x: x['region'])
        df['year_month'] = df['_id'].apply(lambda x: x['year_month'])
        df['quarter'] = df['_id'].apply(lambda x: x['quarter'])
        df = df.drop('_id', axis=1)
        
        print("ğŸ“Š × ×™×ª×•×— ××›×™×¨×•×ª ×œ×¤×™ ×—×•×“×© ×•××–×•×¨:")
        print(df.head())
        
        # ×—×™×©×•×‘ ××—×•×–×™ ×¦××™×—×”
        df = df.sort_values(['region', 'year_month'])
        df['growth_rate'] = df.groupby('region')['total_sales'].pct_change() * 100
        
        print("\nğŸ“ˆ ××—×•×–×™ ×¦××™×—×” ×œ×¤×™ ××–×•×¨:")
        growth_summary = df.groupby('region')['growth_rate'].agg(['mean', 'std']).round(2)
        print(growth_summary)
    
    return df

# ×©×™××•×©:
sales_df = sales_analysis_by_month_region(dal)
```

#### ×ª×¨×—×™×©: × ×™×ª×•×— ×¤×¢×™×œ×•×ª ××©×ª××©×™× ×¢× ×¡×˜×˜×™×¡×˜×™×§×•×ª ××ª×§×“××•×ª
```python
def user_activity_advanced_analysis(dal):
    """× ×™×ª×•×— ×¤×¢×™×œ×•×ª ××©×ª××©×™× ××ª×§×“×"""
    
    pipeline = [
        # ×¡×™× ×•×Ÿ ×œ×—×•×“×© ×”××—×¨×•×Ÿ
        {
            "$match": {
                "timestamp": {"$gte": "2024-01-01"},
                "action_type": {"$ne": "login"}  # ×œ× ×›×•×œ×œ ×”×ª×—×‘×¨×•×™×•×ª
            }
        },
        
        # ×¤×™×¨×•×§ ×œ×¤×™ ×ª××¨×™×›×™× ×•×©×¢×•×ª
        {
            "$addFields": {
                "date": {
                    "$dateToString": {
                        "format": "%Y-%m-%d",
                        "date": {"$dateFromString": {"dateString": "$timestamp"}}
                    }
                },
                "hour": {
                    "$hour": {"$dateFromString": {"dateString": "$timestamp"}}
                },
                "day_of_week": {
                    "$dayOfWeek": {"$dateFromString": {"dateString": "$timestamp"}}
                }
            }
        },
        
        # ×§×™×‘×•×¥ ×œ×¤×™ ××©×ª××©
        {
            "$group": {
                "_id": "$user_id",
                "total_actions": {"$sum": 1},
                "unique_dates": {"$addToSet": "$date"},
                "unique_action_types": {"$addToSet": "$action_type"},
                "peak_hour": {"$max": "$hour"},
                "first_action": {"$min": "$timestamp"},
                "last_action": {"$max": "$timestamp"},
                "weekend_actions": {
                    "$sum": {
                        "$cond": [
                            {"$in": ["$day_of_week", [1, 7]]},  # ×¨××©×•×Ÿ ×•×©×‘×ª
                            1,
                            0
                        ]
                    }
                }
            }
        },
        
        # ×”×•×¡×¤×ª ××˜×¨×™×§×•×ª ××—×•×©×‘×•×ª
        {
            "$addFields": {
                "active_days": {"$size": "$unique_dates"},
                "action_variety": {"$size": "$unique_action_types"},
                "weekend_ratio": {
                    "$divide": ["$weekend_actions", "$total_actions"]
                }
            }
        },
        
        # ×¡×™×•×•×’ ××©×ª××©×™× ×œ×¤×™ ×¨××ª ×¤×¢×™×œ×•×ª
        {
            "$addFields": {
                "user_type": {
                    "$switch": {
                        "branches": [
                            {
                                "case": {"$gte": ["$total_actions", 100]},
                                "then": "high_activity"
                            },
                            {
                                "case": {"$gte": ["$total_actions", 20]},
                                "then": "medium_activity"
                            }
                        ],
                        "default": "low_activity"
                    }
                }
            }
        },
        
        # ××™×•×Ÿ ×œ×¤×™ ×›××•×ª ×¤×¢×•×œ×•×ª
        {"$sort": {"total_actions": -1}}
    ]
    
    results = dal.aggregate_data("user_activities", pipeline)
    df = pd.DataFrame(results)
    
    if not df.empty:
        df['user_id'] = df['_id']
        df = df.drop('_id', axis=1)
        
        print("ğŸ‘¥ × ×™×ª×•×— ×¤×¢×™×œ×•×ª ××©×ª××©×™×:")
        print(f"   ×¡×”\"×› ××©×ª××©×™×: {len(df)}")
        
        # ×”×ª×¤×œ×’×•×ª ×œ×¤×™ ×¡×•×’ ×¤×¢×™×œ×•×ª
        activity_dist = df['user_type'].value_counts()
        print(f"\nğŸ“Š ×”×ª×¤×œ×’×•×ª ×¤×¢×™×œ×•×ª:")
        for activity_type, count in activity_dist.items():
            print(f"   {activity_type}: {count} ××©×ª××©×™×")
        
        # ×¡×˜×˜×™×¡×˜×™×§×•×ª ××ª×§×“××•×ª
        print(f"\nğŸ“ˆ ×¡×˜×˜×™×¡×˜×™×§×•×ª:")
        print(f"   ×××•×¦×¢ ×¤×¢×•×œ×•×ª ×œ××©×ª××©: {df['total_actions'].mean():.1f}")
        print(f"   ×××•×¦×¢ ×™××™ ×¤×¢×™×œ×•×ª: {df['active_days'].mean():.1f}")
        print(f"   ××—×•×– ×¤×¢×™×œ×•×ª ×‘×¡×•×¤\"×©: {df['weekend_ratio'].mean()*100:.1f}%")
        
        # ××©×ª××©×™× ××•×‘×™×œ×™×
        top_users = df.head(5)
        print(f"\nğŸ† 5 ×”××©×ª××©×™× ×”×¤×¢×™×œ×™× ×‘×™×•×ª×¨:")
        for _, user in top_users.iterrows():
            print(f"   {user['user_id']}: {user['total_actions']} ×¤×¢×•×œ×•×ª, {user['active_days']} ×™××™×")
    
    return df

# ×©×™××•×©:
activity_df = user_activity_advanced_analysis(dal)
```

### 3. ×¤×™×™×¤×œ×™×™× ×™× ××•×ª×××™× ×œ××‘×—×Ÿ

#### ×¤×™×™×¤×œ×™×™×Ÿ ×œ×—×™×©×•×‘ TOP N
```python
def get_top_performers(dal, collection_name, 
                      metric_field, group_by_field=None,
                      top_n=10, filters=None):
    """×¤×™×™×¤×œ×™×™×Ÿ ×’× ×¨×™ ×œ××¦×™××ª TOP N ×‘×™×¦×•×¢×™×"""
    
    pipeline = []
    
    # ×¡×™× ×•×Ÿ ××•×¤×¦×™×•× ×œ×™
    if filters:
        pipeline.append({"$match": filters})
    
    if group_by_field:
        # ×¢× ×§×™×‘×•×¥
        pipeline.extend([
            {
                "$group": {
                    "_id": f"${group_by_field}",
                    "total_metric": {"$sum": f"${metric_field}"},
                    "avg_metric": {"$avg": f"${metric_field}"},
                    "count": {"$sum": 1},
                    "max_single": {"$max": f"${metric_field}"},
                    "min_single": {"$min": f"${metric_field}"}
                }
            },
            {"$sort": {"total_metric": -1}},
            {"$limit": top_n}
        ])
    else:
        # ×‘×œ×™ ×§×™×‘×•×¥ - TOP records
        pipeline.extend([
            {"$sort": {metric_field: -1}},
            {"$limit": top_n}
        ])
    
    results = dal.aggregate_data(collection_name, pipeline)
    
    print(f"ğŸ† TOP {len(results)} ×‘-{metric_field}:")
    for i, item in enumerate(results, 1):
        if group_by_field:
            print(f"   {i}. {item['_id']}: {item['total_metric']:.2f}")
        else:
            key_field = 'name' if 'name' in item else list(item.keys())[0]
            print(f"   {i}. {item.get(key_field, 'N/A')}: {item[metric_field]}")
    
    return results

# ×“×•×’×××•×ª ×©×™××•×©:
# TOP ×¡×˜×•×“× ×˜×™× ×œ×¤×™ GPA
top_students = get_top_performers(dal, "students", "gpa", filters={"active": True})

# TOP ×§×•×¨×¡×™× ×œ×¤×™ ×××•×¦×¢ ×¦×™×•× ×™×
top_courses = get_top_performers(dal, "students", "gpa", group_by_field="course", top_n=5)

# TOP ××›×™×¨×•×ª ×œ×¤×™ ××–×•×¨
top_regions = get_top_performers(dal, "sales", "amount", group_by_field="region")
```

#### ×¤×™×™×¤×œ×™×™×Ÿ ×œ×–×™×”×•×™ ×—×¨×™×’×•×ª (Outliers)
```python
def detect_outliers_aggregation(dal, collection_name, numeric_field, group_by=None):
    """×–×™×”×•×™ ×—×¨×™×’×•×ª ×‘×××¦×¢×•×ª ××’×¨×’×¦×™×”"""
    
    pipeline = [
        # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×‘×¡×™×¡×™×•×ª
        {
            "$group": {
                "_id": f"${group_by}" if group_by else None,
                "values": {"$push": f"${numeric_field}"},
                "mean": {"$avg": f"${numeric_field}"},
                "count": {"$sum": 1},
                "min": {"$min": f"${numeric_field}"},
                "max": {"$max": f"${numeric_field}"}
            }
        },
        
        # ×—×™×©×•×‘ ×¡×˜×™×™×ª ×ª×§×Ÿ (×§×™×¨×•×‘)
        {
            "$addFields": {
                "variance_prep": {
                    "$map": {
                        "input": "$values",
                        "as": "value",
                        "in": {
                            "$pow": [{"$subtract": ["$value", "$mean"]}, 2]
                        }
                    }
                }
            }
        },
        
        {
            "$addFields": {
                "variance": {"$avg": "$variance_prep"},
                "std_dev": {"$sqrt": {"$avg": "$variance_prep"}}
            }
        },
        
        # ×”×’×“×¨×ª ×’×‘×•×œ×•×ª ×œ×—×¨×™×’×•×ª (mean Â± 2*std)
        {
            "$addFields": {
                "lower_bound": {"$subtract": ["$mean", {"$multiply": [2, "$std_dev"]}]},
                "upper_bound": {"$add": ["$mean", {"$multiply": [2, "$std_dev"]}]},
                "outliers": {
                    "$filter": {
                        "input": "$values",
                        "as": "value",
                        "cond": {
                            "$or": [
                                {"$lt": ["$value", {"$subtract": ["$mean", {"$multiply": [2, "$std_dev"]}]}]},
                                {"$gt": ["$value", {"$add": ["$mean", {"$multiply": [2, "$std_dev"]}]}]}
                            ]
                        }
                    }
                }
            }
        },
        
        {
            "$addFields": {
                "outlier_count": {"$size": "$outliers"},
                "outlier_percentage": {
                    "$multiply": [
                        {"$divide": [{"$size": "$outliers"}, "$count"]},
                        100
                    ]
                }
            }
        }
    ]
    
    results = dal.aggregate_data(collection_name, pipeline)
    
    for result in results:
        group_name = result['_id'] if result['_id'] else "×›×œ ×”× ×ª×•× ×™×"
        print(f"\nğŸ“Š ×—×¨×™×’×•×ª ×‘-{numeric_field} ×¢×‘×•×¨ {group_name}:")
        print(f"   ×××•×¦×¢: {result['mean']:.2f}")
        print(f"   ×¡×˜×™×™×ª ×ª×§×Ÿ: {result['std_dev']:.2f}")
        print(f"   ×˜×•×•×— × ×•×¨××œ×™: {result['lower_bound']:.2f} - {result['upper_bound']:.2f}")
        print(f"   ×—×¨×™×’×•×ª: {result['outlier_count']} ××ª×•×š {result['count']} ({result['outlier_percentage']:.1f}%)")
        if result['outliers']:
            print(f"   ×¢×¨×›×™ ×—×¨×™×’×•×ª: {result['outliers']}")
    
    return results

# ×©×™××•×©:
outliers = detect_outliers_aggregation(dal, "students", "gpa", group_by="course")
```

---

## ×©×™×œ×•×‘ ×—×›× ×©×œ MongoDB + Pandas

### 1. ××¡×˜×¨×˜×’×™×™×ª "Reduce then Analyze"
```python
def smart_data_analysis(dal, collection_name, analysis_type="summary"):
    """×©×™×œ×•×‘ ×—×›×: ×¦××¦×•× ×‘-MongoDB, × ×™×ª×•×— ×‘-Pandas"""
    
    if analysis_type == "summary":
        # ×©×œ×‘ 1: ×¦××¦×•× ×¢× MongoDB
        pipeline = [
            {"$match": {"active": True}},
            {"$group": {
                "_id": {
                    "course": "$course",
                    "year": "$year"
                },
                "student_count": {"$sum": 1},
                "avg_gpa": {"$avg": "$gpa"},
                "total_credits": {"$sum": "$credits"},
                "students": {"$push": {
                    "name": "$name",
                    "gpa": "$gpa",
                    "credits": "$credits"
                }}
            }}
        ]
        
        # ×§×‘×œ×ª × ×ª×•× ×™× ××¦×•××¦××™×
        mongo_results = dal.aggregate_data(collection_name, pipeline)
        
        # ×©×œ×‘ 2: ×”××¨×” ×œ-Pandas ×œ× ×™×ª×•×— ××ª×§×“×
        summary_data = []
        detailed_data = []
        
        for result in mongo_results:
            # × ×ª×•× ×™ ×¡×™×›×•×
            summary_data.append({
                'course': result['_id']['course'],
                'year': result['_id']['year'],
                'student_count': result['student_count'],
                'avg_gpa': result['avg_gpa'],
                'total_credits': result['total_credits'],
                'credits_per_student': result['total_credits'] / result['student_count']
            })
            
            # × ×ª×•× ×™× ××¤×•×¨×˜×™×
            for student in result['students']:
                detailed_data.append({
                    'course': result['_id']['course'],
                    'year': result['_id']['year'],
                    'name': student['name'],
                    'gpa': student['gpa'],
                    'credits': student['credits']
                })
        
        summary_df = pd.DataFrame(summary_data)
        detailed_df = pd.DataFrame(detailed_data)
        
        # ×©×œ×‘ 3: × ×™×ª×•×— ××ª×§×“× ×‘-Pandas
        print("ğŸ“Š × ×™×ª×•×— ××ª×§×“×:")
        
        # ××¦×™××ª ×”×§×•×¨×¡ ×”×˜×•×‘ ×‘×™×•×ª×¨ ×‘×›×œ ×©× ×”
        best_by_year = summary_df.loc[summary_df.groupby('year')['avg_gpa'].idxmax()]
        print("\nğŸ† ×”×§×•×¨×¡ ×”×˜×•×‘ ×‘×™×•×ª×¨ ×‘×›×œ ×©× ×”:")
        print(best_by_year[['year', 'course', 'avg_gpa']])
        
        # ×§×•×¨×œ×¦×™×” ×‘×™×Ÿ ×’×•×“×œ ×”×§×•×¨×¡ ×œ×××•×¦×¢
        correlation = summary_df['student_count'].corr(summary_df['avg_gpa'])
        print(f"\nğŸ“ˆ ×§×•×¨×œ×¦×™×” ×‘×™×Ÿ ×’×•×“×œ ×§×•×¨×¡ ×œ×××•×¦×¢: {correlation:.3f}")
        
        # ×”×ª×¤×œ×’×•×ª GPA
        gpa_distribution = detailed_df['gpa'].describe()
        print(f"\nğŸ“‹ ×”×ª×¤×œ×’×•×ª GPA:")
        print(gpa_distribution)
        
        return summary_df, detailed_df
    
    elif analysis_type == "time_series":
        # × ×™×ª×•×— ×–×× ×™ - ×“×•×’××” ×œ××›×™×¨×•×ª
        pipeline = [
            {"$match": {"status": "completed"}},
            {"$addFields": {
                "date": {"$dateFromString": {"dateString": "$sale_date"}},
                "year_month": {
                    "$dateToString": {
                        "format": "%Y-%m",
                        "date": {"$dateFromString": {"dateString": "$sale_date"}}
                    }
                }
            }},
            {"$group": {
                "_id": "$year_month",
                "total_sales": {"$sum": "$amount"},
                "transaction_count": {"$sum": 1},
                "avg_transaction": {"$avg": "$amount"}
            }},
            {"$sort": {"_id": 1}}
        ]
        
        mongo_results = dal.aggregate_data(collection_name, pipeline)
        df = pd.DataFrame(mongo_results)
        
        if not df.empty:
            df['month'] = df['_id']
            df['month'] = pd.to_datetime(df['month'])
            df = df.set_index('month').drop('_id', axis=1)
            
            # ×—×™×©×•×‘ ××’××•×ª
            df['sales_trend'] = df['total_sales'].pct_change()
            df['sales_ma3'] = df['total_sales'].rolling(window=3).mean()
            
            # ×–×™×”×•×™ ×¢×•× ×ª×™×•×ª
            df['month_num'] = df.index.month
            seasonal = df.groupby('month_num')['total_sales'].mean()
            
            print("ğŸ“ˆ × ×™×ª×•×— ×–×× ×™:")
            print(f"   ×¦××™×—×” ×××•×¦×¢×ª: {df['sales_trend'].mean()*100:.1f}%")
            print(f"   ×—×•×“×© ×”×›×™ ×—×–×§: {seasonal.idxmax()} (×××•×¦×¢: {seasonal.max():.0f})")
            print(f"   ×—×•×“×© ×”×›×™ ×—×œ×©: {seasonal.idxmin()} (×××•×¦×¢: {seasonal.min():.0f})")
        
        return df

# ×©×™××•×©:
summary_df, detailed_df = smart_data_analysis(dal, "students", "summary")
time_series_df = smart_data_analysis(dal, "sales", "time_series")
```

### 2. ×˜×›× ×™×§×ª "Progressive Loading"
```python
def progressive_data_loading(dal, collection_name, 
                           initial_filters=None,
                           drill_down_fields=None):
    """×˜×¢×™× ×” ×”×“×¨×’×ª×™×ª - ××§×¨×™×¢ ×œ×¤×¨×˜×™"""
    
    print("ğŸ” ×©×œ×‘ 1: ×¡×§×™×¨×” ×›×œ×œ×™×ª")
    
    # ×©×œ×‘ 1: ××™×“×¢ ×›×œ×œ×™
    total_count = dal.count_documents(collection_name, initial_filters)
    sample = dal.find_one_document(collection_name, initial_filters or {})
    
    print(f"   ×¡×”\"×› ×¨×©×•××•×ª: {total_count:,}")
    print(f"   ×©×“×•×ª ×–××™× ×™×: {list(sample.keys()) if sample else 'N/A'}")
    
    # ×©×œ×‘ 2: ×”×ª×¤×œ×’×•×™×•×ª ×‘×¡×™×¡×™×•×ª
    if drill_down_fields:
        print("\nğŸ“Š ×©×œ×‘ 2: ×”×ª×¤×œ×’×•×™×•×ª")
        
        distributions = {}
        for field in drill_down_fields:
            if sample and field in sample:
                unique_values = dal.get_distinct_values(collection_name, field)
                distributions[field] = unique_values
                print(f"   {field}: {len(unique_values)} ×¢×¨×›×™× ×™×™×—×•×“×™×™×")
                if len(unique_values) <= 10:
                    print(f"      ×¢×¨×›×™×: {unique_values}")
    
    # ×©×œ×‘ 3: ×‘×—×™×¨×” ×—×›××” ×©×œ × ×ª×•× ×™× ×œ×˜×¢×™× ×”
    print("\nğŸ“¦ ×©×œ×‘ 3: ×‘×—×™×¨×ª × ×ª×•× ×™× ×œ×˜×¢×™× ×” ××¤×•×¨×˜×ª")
    
    if total_count > 10000:
        print("   ××¢×¨×š × ×ª×•× ×™× ×’×“×•×œ - ×‘×•×—×¨ ×“×’×™××” ××™×™×¦×’×ª")
        
        # ××’×¨×’×¦×™×” ×œ×“×’×™××” ××™×™×¦×’×ª
        sampling_pipeline = [
            {"$match": initial_filters or {}},
            {"$sample": {"size": 5000}},  # ×“×’×™××” ××§×¨××™×ª
            {"$project": {"_id": 1}}  # ×¨×§ ××–×”×™×
        ]
        
        sample_ids = [doc['_id'] for doc in dal.aggregate_data(collection_name, sampling_pipeline)]
        
        # ×˜×¢×™× ×ª ×”×“×’×™××”
        detailed_data = dal.find_documents(
            collection_name,
            {"_id": {"$in": sample_ids}}
        )
        
        print(f"   × ×˜×¢× ×” ×“×’×™××” ×©×œ {len(detailed_data)} ×¨×©×•××•×ª")
        
    else:
        print("   ××¢×¨×š × ×ª×•× ×™× ×§×˜×Ÿ - ×˜×•×¢×Ÿ ×”×›×œ")
        detailed_data = dal.find_documents(collection_name, initial_filters)
    
    # ×©×œ×‘ 4: ×”××¨×” ×œ-DataFrame ×•× ×™×ª×•×— ×¨××©×•× ×™
    df = pd.DataFrame(detailed_data)
    
    if not df.empty:
        print(f"\nğŸ¼ ×©×œ×‘ 4: DataFrame ××•×›×Ÿ ×¢× {len(df)} ×©×•×¨×•×ª")
        
        # × ×™×ª×•×— ××•×˜×•××˜×™ ×©×œ ×”×©×“×•×ª
        numeric_fields = df.select_dtypes(include=['number']).columns.tolist()
        text_fields = df.select_dtypes(include=['object']).columns.tolist()
        
        print(f"   ×©×“×•×ª ××¡×¤×¨×™×™×: {numeric_fields}")
        print(f"   ×©×“×•×ª ×˜×§×¡×˜: {text_fields}")
        
        # ×¡×˜×˜×™×¡×˜×™×§×•×ª ××”×™×¨×•×ª ×œ×©×“×•×ª ××¡×¤×¨×™×™×
        if numeric_fields:
            print(f"\nğŸ“ˆ ×¡×˜×˜×™×¡×˜×™×§×•×ª ××”×™×¨×•×ª:")
            for field in numeric_fields[:3]:  # ×¨×§ 3 ×”×¨××©×•× ×™×
                mean_val = df[field].mean()
                median_val = df[field].median()
                std_val = df[field].std()
                print(f"   {field}: ×××•×¦×¢={mean_val:.2f}, ×—×¦×™×•×Ÿ={median_val:.2f}, ×¡×˜×™×™×ª ×ª×§×Ÿ={std_val:.2f}")
    
    return df, distributions if drill_down_fields else {}

# ×©×™××•×©:
df, distributions = progressive_data_loading(
    dal, 
    "students",
    initial_filters={"active": True},
    drill_down_fields=["course", "year", "status"]
)
```

### 3. ××ª×›×•× ×™× ××•×›× ×™× ×œ× ×™×ª×•×—
```python
class MongoAnalysisRecipes:
    """××ª×›×•× ×™× ××•×›× ×™× ×œ× ×™×ª×•×—×™× × ×¤×•×¦×™×"""
    
    def __init__(self, dal):
        self.dal = dal
    
    def customer_segmentation(self, collection_name="customers"):
        """×¡×™×’×× ×˜×¦×™×” ×©×œ ×œ×§×•×—×•×ª"""
        
        pipeline = [
            {"$group": {
                "_id": "$customer_id",
                "total_purchases": {"$sum": "$amount"},
                "purchase_count": {"$sum": 1},
                "avg_purchase": {"$avg": "$amount"},
                "first_purchase": {"$min": "$purchase_date"},
                "last_purchase": {"$max": "$purchase_date"}
            }},
            {"$addFields": {
                "days_since_last": {
                    "$divide": [
                        {"$subtract": [{"$dateFromString": {"dateString": "2024-01-01"}}, 
                                     {"$dateFromString": {"dateString": "$last_purchase"}}]},
                        86400000  # ××™×œ×™×©× ×™×•×ª ×œ×™×•×
                    ]
                }
            }}
        ]
        
        mongo_results = self.dal.aggregate_data(collection_name, pipeline)
        df = pd.DataFrame(mongo_results)
        
        if not df.empty:
            df['customer_id'] = df['_id']
            df = df.drop('_id', axis=1)
            
            # ×™×¦×™×¨×ª ×¡×™×’×× ×˜×™×
            df['value_segment'] = pd.qcut(df['total_purchases'], 
                                        q=3, 
                                        labels=['Low', 'Medium', 'High'])
            
            df['frequency_segment'] = pd.qcut(df['purchase_count'], 
                                            q=3, 
                                            labels=['Rare', 'Regular', 'Frequent'])
            
            df['recency_segment'] = pd.qcut(df['days_since_last'], 
                                          q=3, 
                                          labels=['Recent', 'Moderate', 'Old'])
            
            # ×™×¦×™×¨×ª ×¡×™×’×× ×˜ ××©×•×œ×‘
            df['customer_segment'] = (df['value_segment'].astype(str) + '_' + 
                                    df['frequency_segment'].astype(str) + '_' + 
                                    df['recency_segment'].astype(str))
            
            # ×¡×™×›×•× ×¡×™×’×× ×˜×™×
            segment_summary = df.groupby('customer_segment').agg({
                'customer_id': 'count',
                'total_purchases': 'mean',
                'purchase_count': 'mean'
            }).round(2)
            
            print("ğŸ‘¥ ×¡×™×’×× ×˜×¦×™×” ×©×œ ×œ×§×•×—×•×ª:")
            print(segment_summary)
        
        return df
    
    def sales_cohort_analysis(self, collection_name="sales"):
        """× ×™×ª×•×— ×§×•×”×•×¨×˜ ×©×œ ××›×™×¨×•×ª"""
        
        # MongoDB ××’×¨×’×¦×™×” ×œ×§×‘×œ×ª × ×ª×•× ×™ ×‘×¡×™×¡
        pipeline = [
            {"$addFields": {
                "purchase_date": {"$dateFromString": {"dateString": "$purchase_date"}},
                "customer_id": "$customer_id"
            }},
            {"$group": {
                "_id": "$customer_id",
                "first_purchase": {"$min": "$purchase_date"},
                "purchases": {"$push": {
                    "date": "$purchase_date",
                    "amount": "$amount"
                }}
            }},
            {"$addFields": {
                "first_purchase_month": {
                    "$dateToString": {
                        "format": "%Y-%m",
                        "date": "$first_purchase"
                    }
                }
            }}
        ]
        
        mongo_results = self.dal.aggregate_data(collection_name, pipeline)
        
        # ×¢×™×‘×•×“ ×‘-Pandas
        cohort_data = []
        
        for customer in mongo_results:
            first_month = customer['first_purchase_month']
            first_date = pd.to_datetime(customer['first_purchase'])
            
            for purchase in customer['purchases']:
                purchase_date = pd.to_datetime(purchase['date'])
                months_since = ((purchase_date.year - first_date.year) * 12 + 
                              purchase_date.month - first_date.month)
                
                cohort_data.append({
                    'customer_id': customer['_id'],
                    'cohort_month': first_month,
                    'period_number': months_since,
                    'amount': purchase['amount']
                })
        
        df = pd.DataFrame(cohort_data)
        
        if not df.empty:
            # ×™×¦×™×¨×ª ×˜×‘×œ×ª ×§×•×”×•×¨×˜
            cohort_table = df.groupby(['cohort_month', 'period_number'])['customer_id'].nunique().reset_index()
            cohort_table = cohort_table.pivot(index='cohort_month', 
                                             columns='period_number', 
                                             values='customer_id')
            
            # ×—×™×©×•×‘ ×©×™×¢×•×¨×™ ×”×—×–×¨×”
            cohort_sizes = cohort_table.iloc[:, 0]
            retention_table = cohort_table.divide(cohort_sizes, axis=0)
            
            print("ğŸ“ˆ × ×™×ª×•×— ×§×•×”×•×¨×˜ - ×©×™×¢×•×¨×™ ×”×—×–×¨×”:")
            print(retention_table.round(3))
        
        return df, cohort_table
    
    def product_recommendation_data(self, collection_name="order_items"):
        """×”×›× ×ª × ×ª×•× ×™× ×œ×”××œ×¦×•×ª ××•×¦×¨×™×"""
        
        # ×—×™×¤×•×© ××•×¦×¨×™× ×©× ×§× ×• ×™×—×“
        pipeline = [
            {"$group": {
                "_id": "$order_id",
                "products": {"$addToSet": "$product_id"}
            }},
            {"$match": {
                "products": {"$size": {"$gte": 2}}  # ×¨×§ ×”×–×× ×•×ª ×¢× 2+ ××•×¦×¨×™×
            }},
            {"$unwind": "$products"},
            {"$group": {
                "_id": "$products",
                "co_purchased_with": {"$addToSet": "$_id"}
            }}
        ]
        
        # ×–×” ××•×¨×›×‘ ××“×™ ×œ-MongoDB, ×¢×“×™×£ ×‘-Pandas
        # × ×§×— ××ª ×”× ×ª×•× ×™× ×”×’×•×œ××™×™×
        raw_data = self.dal.find_documents(collection_name)
        df = pd.DataFrame(raw_data)
        
        if not df.empty:
            # ×™×¦×™×¨×ª ××˜×¨×™×¦×ª co-occurrence
            order_product = df.groupby('order_id')['product_id'].apply(list).reset_index()
            
            from itertools import combinations
            
            co_occurrence = {}
            for _, row in order_product.iterrows():
                products = row['product_id']
                if len(products) > 1:
                    for combo in combinations(products, 2):
                        key = tuple(sorted(combo))
                        co_occurrence[key] = co_occurrence.get(key, 0) + 1
            
            # ×”××¨×” ×œ-DataFrame
            co_df = pd.DataFrame([
                {'product_a': k[0], 'product_b': k[1], 'frequency': v}
                for k, v in co_occurrence.items()
            ])
            
            # ××™×•×Ÿ ×œ×¤×™ ×ª×›×™×¤×•×ª
            co_df = co_df.sort_values('frequency', ascending=False)
            
            print("ğŸ›’ ××•×¦×¨×™× ×”× ×§× ×™× ×™×—×“ (TOP 10):")
            print(co_df.head(10))
        
        return co_df

# ×©×™××•×©:
recipes = MongoAnalysisRecipes(dal)

# ×¡×™×’×× ×˜×¦×™×”
customer_segments = recipes.customer_segmentation("purchases")

# × ×™×ª×•×— ×§×•×”×•×¨×˜
cohort_df, cohort_table = recipes.sales_cohort_analysis("sales")

# ×”××œ×¦×•×ª ××•×¦×¨×™×
recommendations = recipes.product_recommendation_data("order_items")
```

---

## ×¡×™×›×•×: ××ª×™ ×œ×”×©×ª××© ×‘××”?

### ğŸ¯ ×›×œ×œ ×”×—×œ×˜×” ××”×™×¨:

| ×”×ª×¨×—×™×© | ×›×œ×™ ××•××œ×¥ | ×¡×™×‘×” |
|---------|-----------|-------|
| ×¡×›×™××ª ×¨×©×•××•×ª ×’×“×•×œ×” (>50K) | MongoDB Aggregation | ×¦××¦×•× × ×ª×•× ×™× ×œ×¤× ×™ ×©×œ×™×¤×” |
| ×—×™×©×•×‘×™× ×¤×©×˜×™× (×¡×›×•×, ×××•×¦×¢) | MongoDB Aggregation | ××”×™×¨ ×•×™×¢×™×œ |
| × ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ××•×¨×›×‘ | Pandas | ×’××™×©×•×ª ×•×›×œ×™× ××ª×§×“××™× |
| ×§×•×¨×œ×¦×™×•×ª ×•×¨×’×¨×¡×™×•×ª | Pandas | ××•×ª×× ×œ× ×™×ª×•×— ××ª××˜×™ |
| ×××’×¨ × ×ª×•× ×™× ×§×˜×Ÿ (<10K) | Pandas | ×¤×©×•×˜ ×™×•×ª×¨ |
| ×“×•×—×•×ª ×‘×–××Ÿ ×××ª | MongoDB Aggregation | ××”×™×¨×•×ª |
| ×¢×™×‘×•×“ × ×ª×•× ×™× ×—×“-×¤×¢××™ | Pandas | ×’××™×©×•×ª |

### ğŸ”§ ×”×§×•×“ ×”××™× ×™××œ×™ ×œ××‘×—×Ÿ:

```python
# 1. ×—×™×‘×•×¨ ×•×˜×¢×™× ×” ××”×™×¨×”
dal = AtlasDAL("connection_string", "db_name")
dal.connect()

# 2. ×‘×“×™×§×” ×¨××©×•× ×™×ª
sample = dal.find_one_document("collection", {})
count = dal.count_documents("collection")
print(f"××‘× ×”: {list(sample.keys())}, ×¡×”\"×›: {count}")

# 3. ××’×¨×’×¦×™×” ××”×™×¨×” (×× ×¦×¨×™×š ×¦××¦×•×)
if count > 10000:
    pipeline = [
        {"$match": {"active": True}},
        {"$group": {"_id": "$category", "count": {"$sum": 1}}},
        {"$sort": {"count": -1}}
    ]
    summary = dal.aggregate_data("collection", pipeline)
    df = pd.DataFrame(summary)
else:
    # 4. ×˜×¢×™× ×” ×™×©×™×¨×” (×× ×§×˜×Ÿ)
    data = dal.find_documents("collection", {"active": True})
    df = pd.DataFrame(data)

# 5. × ×™×ª×•×— ×‘-Pandas
print(df.describe())
print(df.groupby('category').size())

dal.disconnect()
```

**×–×›×•×¨: ×ª××™×“ ×ª×ª×—×™×œ ×¢× ×‘×“×™×§×” ×¨××©×•× ×™×ª ×œ×¤× ×™ ×©×ª×—×œ×™×˜ ××™×š ×œ×”××©×™×š!** ğŸ¯